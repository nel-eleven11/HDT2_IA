{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAS8TTYatHVE"
      },
      "source": [
        "# Task 1 - Preguntas Teóricas\n",
        "\n",
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht8Buw06tHVH"
      },
      "source": [
        "1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
        "\n",
        "El proceso de decisión de Markov (MDP) es un marco matemático utilizado para modelar la toma de decisiones en entornos donde los resultados dependen tanto del azar como de las acciones de un agente. Se define mediante una tupla (S,A,P,R), cuyos componentes son:\n",
        "\n",
        "- S (Espacio de Estados): Conjunto de todos los estados posibles en los que puede encontrarse el sistema.\n",
        "- A (Espacio de Acciones): Conjunto de todas las acciones posibles que el agente puede ejecutar en cada estado.\n",
        "- P (Probabilidad de Transición de Estado): Función que define la probabilidad de moverse de un estado a otro dado que se ha tomado una acción específica.\n",
        "- R (Función de Recompensa): Asigna un valor numérico a cada par estado-acción, reflejando la conveniencia de dicha acción en un estado determinado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "gDYYZy7TtHVI"
      },
      "source": [
        "2. Describa cuál es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas\n",
        "en el contexto de los PDM.\n",
        "\n",
        "- Política: Es una regla que nos da acciones a poder tomar en cada estado del entorno. Esta se puede llegar a representar como una función que se asigna a cada uno de los estados una acción o distribución de una probabilidad sobre dichas acciones. El agente se guía de esta política para interactuar con el ambiente.\n",
        "- Evaluación de políticas: Nos ayuda a calcular la función de valor que se asocia a una política dada. Con una evaluación de políticas determinamos la recompensa acumulada esperada al seguir una política desde un estado en cierta dirección. Se podría decir que evalúa que tan buena es la política en cuanto a rendimiento a largo plazo.\n",
        "- Mejora de políticas: Actualiza o hace ajustes a la política utilizada haciendo uso de la evaluación realizada. La manera en la que se hace uso de la evaluación de políticas es determinando las acciones que pueden incrementar la recompensa esperada en cada estado, con esto se modifica la política para que en cada estado se asegure de seleccionar la acción más óptima.\n",
        "- Iteración de políticas: Con la iteración usamos los dos pasos anteriores haciendo una evaluación de la política actual y después se mejora la política usando los resultados de la evaluación de políticas. Este proceso se repite hasta que se consigue que la política converge, o sea, que no se puede obtener mejoras, esto quiere decir que se llegó a una política óptima."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
        "\n",
        "Cuando hablamos de MDP el facor de descuento es el parametro que hace una valoracion entre las recompensas futuras y las recompensas inmediatas. El factor de descuento se denota como **γ**, donde 0≤γ≤10≤γ≤1. Cuando se hace la valoración se toma en cuenta si **γ** es más cercano a 1 o 0.  \n",
        "\n",
        "- Si **γ** es cercano a 1, el agente prioriza las recompensas futuras.\n",
        "\n",
        "- Si **γ** es cercano a 0, el agente prioriza las recompensas inmediatas.\n",
        "\n",
        "El factor de descuento Gamma afecta a la toma de decisiones de manera significativamente, ya que el agente dependiendo del factor de descuento tomará decisiones de corto plazo o de largo plazo, siendo el **γ** cercano a 0 el que fomenta la explotación o sea que se enfoca en maximizar las recompensas imnmediatas explotando lo que ya conoce y con el **γ** cercano a 1 siendo el que fomenta la exploración, ya que el agente invierte más tiempo para explorar el entorno para buscar las recompensas futuras."
      ],
      "metadata": {
        "id": "mCUzY9N0tdT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.\n",
        "\n",
        "**Diferencias:**\n",
        "- **Valor:**\n",
        "  - **Enfoque:**\n",
        "  Actualiza directamente la funcion de valor V(s).\n",
        "  - **Ecuación principal:**Ecuación de Bellman de optimización.\n",
        "  - **Convergencia:**Más rapida en términos de iteraciones.\n",
        "  - **Implementación:**Generalemente sencilla.\n",
        "- **Politicas:**\n",
        "  - **Enfoque:** Alterna entre evaluación y mejora de políticas.\n",
        "  - **Ecuación principal:** Ecuación de Bellman para evaluación de politicas.\n",
        "  - **Convergencia:**Menos iteraciones, pero más costosas.\n",
        "  - **Implementación:**Más compleja debido a la evaluación de políticas."
      ],
      "metadata": {
        "id": "VPBQ3cI7tj-R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDsgnBZwtHVL"
      },
      "source": [
        "5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala?\n",
        "Discuta los enfoques potenciales para abordar estos desafíos.\n",
        "\n",
        "Algunos de los desafios de la implementación de los MDP pueden ser:\n",
        "- Dimensión del espacio de estados: El espacio de estados en problemas reales puede volverse incluso infinito lo que dificulta el calculo de la función valor o la politica optima se convierte en incalculable.\n",
        "- Dimensión del espacio acciones: Un caso similar al de los espacios de estado, la cantidad de acciones posibles también puede ser muy grande, lo que aumenta la complejidad de la maximización de la ecuación de Bellman.\n",
        "- Curse of dimensionality: Esta \"maldición\" se refiere a que a medida que aumenta el número de variables de estado, se hace más dificil el almacenamiento y también el calculo de la función de valor o la politica.\n",
        "- Conocimiento del modelo: Cuando se aplican MDP a problemas reales asumimos conocer las probabilidades de transición y también las recompensas cuando esto es dificil de estimar o directamente no las conocemos.\n",
        "\n",
        "**Enfoques a utilizar**:\n",
        "- Aproximación de la función de valor exacta V(s) para cada estado, se pueden utilizar métodos de aproximación tales como:\n",
        "  - Aproximación lineal.\n",
        "  - Redes neuronales.\n",
        "\n",
        "- Agregación de estados: Agrupar estados con similitudes en estados más grandes o \"superestados\" para reducir la dimensionalidad del espacio de estados.\n",
        "- Muestreo y simulación: Utilizar tecnicas de muestreo como Monte Carlo o TD - Learning, para asi tener una estimación de la función de valor o la politica sin explorar todo el espacio de estados.\n",
        "- Descomposición del problema: Dividir el problema  en subproblemas para facilitar la resolución de MPDs más pequeños.\n",
        "- Exploración eficiente: Usar exploraciones como e-greedy, SARSA o una Policy Gradient que no requieren de un modelo sumamente explicito del entorno para funcionar bien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3qCjHD-tHVL"
      },
      "source": [
        "# Task 2 - Preguntas Analíticas\n",
        "\n",
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyitjD3EtHVM"
      },
      "source": [
        "1. Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de\n",
        "Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones\n",
        "para la toma de decisiones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a1qF80etHVM"
      },
      "source": [
        "2. Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice\n",
        "estrategias para una toma de decisiones sólida en entornos inciertos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjjH8_LPtHVM"
      },
      "source": [
        "# Task 3 - Preguntas Prácticas\n",
        "\n",
        "Desarrolle un agente básico capaz de resolver un problema simplificado del Proceso de Decisión de Markov (MDP).\n",
        "Considere utilizar un ejemplo bien conocido como el entorno 'Frozen Lake'. Proporcione el código Python para el\n",
        "proceso de toma de decisiones del agente basándose únicamente en los principios de los procesos de decisión de\n",
        "Markov. Recuerde que para este tipo de problema, el ambiente es una matriz de 4x4, y las acciones, pueden ser\n",
        "moverse hacia arriba, abajo, derecha, izquierda. Considere que el punto inicial siempre estará en la esquina opuesta\n",
        "del punto de meta. Es decir, puede tener hasta 4 configuraciones diferentes. Por ejemplo, el punto inicial puede estar\n",
        "en la coordenada (0, 0) y el punto de meta en la coordenada en la coordenada (3, 3). Además, la posición de los hoyos\n",
        "debe ser determinada aleatoriamente y no debe superar el ser más de 3. Es decir, si aleatoriamente se decide que\n",
        "sean 2 posiciones de hoyo, las coordenadas de estas deben ser determinadas de forma aleatoria. Asegúrese de usar\n",
        "“seed” para que sus resultados sean consistentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rxu-9Kl0kNPE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar la semilla para resultados consistentes\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "gXq3X65omiyC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tamaño del grid\n",
        "grid_size = 4\n",
        "\n",
        "# Definir posibles configuraciones: inicio y meta en esquinas opuestas\n",
        "configs = [((0, 0), (3, 3)),\n",
        "           ((0, 3), (3, 0)),\n",
        "           ((3, 0), (0, 3)),\n",
        "           ((3, 3), (0, 0))]\n",
        "start, goal = random.choice(configs)\n",
        "print(f\"Configuración inicial: {start}\")\n",
        "print(f\"Configuración final: {goal}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tto-twcrml53",
        "outputId": "23466aab-a2d0-48d1-87bb-5708654dc951"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuración inicial: (0, 0)\n",
            "Configuración final: (3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determinar aleatoriamente el número de hoyos (entre 0 y 3)\n",
        "n_holes = random.randint(1, 3)\n",
        "print(f\"Número de hoyos: {n_holes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXpagxJjmpp5",
        "outputId": "21700f45-ec89-4550-bd84-48336c393693"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de hoyos: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar posiciones de hoyo aleatoriamente, evitando el inicio y la meta\n",
        "holes = set()\n",
        "while len(holes) < n_holes:\n",
        "    pos = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
        "    if pos != start and pos != goal:\n",
        "        holes.add(pos)\n",
        "holes = list(holes)\n",
        "\n",
        "print(f\"Posiciones de los hoyos: {holes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijgm8a3lmtCi",
        "outputId": "aa3f69ee-bb86-4061-bc82-53e42d0c147c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posiciones de los hoyos: [(1, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear representación del entorno\n",
        "# 'S' -> Start, 'G' -> Goal, 'H' -> Hoyo, 'F' -> Frozen (superficie segura)\n",
        "grid = [['F' for _ in range(grid_size)] for _ in range(grid_size)]\n",
        "grid[start[0]][start[1]] = 'S'\n",
        "grid[goal[0]][goal[1]] = 'G'\n",
        "for h in holes:\n",
        "    grid[h[0]][h[1]] = 'H'\n",
        "\n",
        "print(\"Configuración del entorno (grid):\")\n",
        "for row in grid:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6M7btKXm2cZ",
        "outputId": "0c7a8846-c75c-45a5-8536-40be20891552"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuración del entorno (grid):\n",
            "['S', 'F', 'F', 'F']\n",
            "['F', 'H', 'F', 'F']\n",
            "['F', 'F', 'F', 'F']\n",
            "['F', 'F', 'F', 'G']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir las acciones y sus efectos (movimientos deterministas)\n",
        "actions = {\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0),\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}\n",
        "action_list = list(actions.keys())"
      ],
      "metadata": {
        "id": "nEHQ9HQ2m4f6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función que determina si un estado es terminal (meta o hoyo)\n",
        "def is_terminal(state):\n",
        "    return state == goal or state in holes\n",
        "\n",
        "# Función de transición: dada una acción en un estado, retorna el siguiente estado\n",
        "def next_state(state, action):\n",
        "    if is_terminal(state):\n",
        "        return state\n",
        "    move = actions[action]\n",
        "    new_state = (state[0] + move[0], state[1] + move[1])\n",
        "    # Verificar límites del grid\n",
        "    if 0 <= new_state[0] < grid_size and 0 <= new_state[1] < grid_size:\n",
        "        return new_state\n",
        "    else:\n",
        "        return state\n",
        "\n",
        "# Función de recompensa:\n",
        "# +1 al alcanzar la meta, -1 si cae en un hoyo, y una pequeña penalización en cada paso\n",
        "def get_reward(state, next_state):\n",
        "    if next_state == goal:\n",
        "        return 1.0\n",
        "    elif next_state in holes:\n",
        "        return -1.0\n",
        "    else:\n",
        "        return -0.04"
      ],
      "metadata": {
        "id": "m-nkRx6_nXjI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construir el espacio de estados: todas las celdas del grid\n",
        "states = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n",
        "\n",
        "# Parámetros para Value Iteration\n",
        "gamma = 0.9      # Factor de descuento\n",
        "theta = 1e-4     # Umbral para convergencia\n",
        "\n",
        "# Inicializar la función de valor (V) para cada estado\n",
        "V = {s: 0 for s in states}"
      ],
      "metadata": {
        "id": "Qq-_4I4gnVEF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UrBehGBhkNPG"
      },
      "outputs": [],
      "source": [
        "# Algoritmo de Value Iteration\n",
        "while True:\n",
        "    delta = 0\n",
        "    for s in states:\n",
        "        if is_terminal(s):\n",
        "            continue\n",
        "        v = V[s]\n",
        "        values = []\n",
        "        for a in action_list:\n",
        "            ns = next_state(s, a)\n",
        "            r = get_reward(s, ns)\n",
        "            values.append(r + gamma * V[ns])\n",
        "        V[s] = max(values)\n",
        "        delta = max(delta, abs(v - V[s]))\n",
        "    if delta < theta:\n",
        "        break\n",
        "\n",
        "# Derivar la política óptima a partir de la función de valor\n",
        "policy = {}\n",
        "for s in states:\n",
        "    if is_terminal(s):\n",
        "        policy[s] = None\n",
        "    else:\n",
        "        best_action = None\n",
        "        best_value = -float('inf')\n",
        "        for a in action_list:\n",
        "            ns = next_state(s, a)\n",
        "            r = get_reward(s, ns)\n",
        "            value = r + gamma * V[ns]\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = a\n",
        "        policy[s] = best_action"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir la función de valor óptima\n",
        "print(\"\\nFunción de Valor Óptima:\")\n",
        "for i in range(grid_size):\n",
        "    row_values = [round(V[(i, j)], 2) for j in range(grid_size)]\n",
        "    print(row_values)\n",
        "\n",
        "# Imprimir la política óptima en formato grid\n",
        "print(\"\\nPolítica Óptima (acción a tomar en cada estado):\")\n",
        "policy_grid = [['' for _ in range(grid_size)] for _ in range(grid_size)]\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        if (i, j) == start:\n",
        "            policy_grid[i][j] = 'S'\n",
        "        elif (i, j) == goal:\n",
        "            policy_grid[i][j] = 'G'\n",
        "        elif (i, j) in holes:\n",
        "            policy_grid[i][j] = 'H'\n",
        "        else:\n",
        "            # Abreviar la acción: up: U, down: D, left: L, right: R\n",
        "            a = policy[(i, j)]\n",
        "            if a == 'up':\n",
        "                policy_grid[i][j] = 'U'\n",
        "            elif a == 'down':\n",
        "                policy_grid[i][j] = 'D'\n",
        "            elif a == 'left':\n",
        "                policy_grid[i][j] = 'L'\n",
        "            elif a == 'right':\n",
        "                policy_grid[i][j] = 'R'\n",
        "            else:\n",
        "                policy_grid[i][j] = ' '\n",
        "for row in policy_grid:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKLecwEWnfXp",
        "outputId": "6036231a-0d50-451b-bc2e-b57fb38495ca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Función de Valor Óptima:\n",
            "[0.43, 0.52, 0.62, 0.73]\n",
            "[0.52, 0, 0.73, 0.86]\n",
            "[0.62, 0.73, 0.86, 1.0]\n",
            "[0.73, 0.86, 1.0, 0]\n",
            "\n",
            "Política Óptima (acción a tomar en cada estado):\n",
            "['S', 'R', 'D', 'D']\n",
            "['D', 'H', 'D', 'D']\n",
            "['D', 'D', 'D', 'D']\n",
            "['R', 'R', 'R', 'G']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulación del agente siguiendo la política óptima\n",
        "current_state = start\n",
        "path = [current_state]\n",
        "max_steps = 50  # Limitar la cantidad de pasos para evitar ciclos infinitos\n",
        "\n",
        "for _ in range(max_steps):\n",
        "    if is_terminal(current_state):\n",
        "        break\n",
        "    a = policy[current_state]\n",
        "    current_state = next_state(current_state, a)\n",
        "    path.append(current_state)\n",
        "\n",
        "print(\"\\nCamino seguido por el agente:\")\n",
        "print(path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgtbFZg3nh3M",
        "outputId": "58c0c493-b861-47ce-9f76-a44ac2d6bb9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Camino seguido por el agente:\n",
            "[(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2), (3, 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ixdUR-tHVQ"
      },
      "source": [
        "### Bibliografía:\n",
        "\n",
        "- Buczyński, R. (2023, 21 septiembre). Understanding Markov Decision Processes - Python in Plain English. Medium. https://python.plainenglish.io/understanding-markov-decision-processes-17e852cd9981"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}