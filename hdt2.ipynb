{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Preguntas Teóricas\n",
    "\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
    "\n",
    "El proceso de decisión de Markov (MDP) es un marco matemático utilizado para modelar la toma de decisiones en entornos donde los resultados dependen tanto del azar como de las acciones de un agente. Se define mediante una tupla (S,A,P,R), cuyos componentes son:\n",
    "\n",
    "- S (Espacio de Estados): Conjunto de todos los estados posibles en los que puede encontrarse el sistema.\n",
    "- A (Espacio de Acciones): Conjunto de todas las acciones posibles que el agente puede ejecutar en cada estado.\n",
    "- P (Probabilidad de Transición de Estado): Función que define la probabilidad de moverse de un estado a otro dado que se ha tomado una acción específica.\n",
    "- R (Función de Recompensa): Asigna un valor numérico a cada par estado-acción, reflejando la conveniencia de dicha acción en un estado determinado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describa cuál es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas\n",
    "en el contexto de los PDM.\n",
    "\n",
    "- Política: Es una regla que nos da acciones a poder tomar en cada estado del entorno. Esta se puede llegar a representar como una función que se asigna a cada uno de los estados una acción o distribución de una probabilidad sobre dichas acciones. El agente se guía de esta política para interactuar con el ambiente.\n",
    "- Evaluación de políticas: Nos ayuda a calcular la función de valor que se asocia a una política dada. Con una evaluación de políticas determinamos la recompensa acumulada esperada al seguir una política desde un estado en cierta dirección. Se podría decir que evalúa que tan buena es la política en cuanto a rendimiento a largo plazo. \n",
    "- Mejora de políticas: Actualiza o hace ajustes a la política utilizada haciendo uso de la evaluación realizada. La manera en la que se hace uso de la evaluación de políticas es determinando las acciones que pueden incrementar la recompensa esperada en cada estado, con esto se modifica la política para que en cada estado se asegure de seleccionar la acción más óptima. \n",
    "- Iteración de políticas: Con la iteración usamos los dos pasos anteriores haciendo una evaluación de la política actual y después se mejora la política usando los resultados de la evaluación de políticas. Este proceso se repite hasta que se consigue que la política converge, o sea, que no se puede obtener mejoras, esto quiere decir que se llegó a una política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala?\n",
    "Discuta los enfoques potenciales para abordar estos desafíos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Preguntas Analíticas\n",
    "\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de\n",
    "Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones\n",
    "para la toma de decisiones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice\n",
    "estrategias para una toma de decisiones sólida en entornos inciertos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Preguntas Prácticas\n",
    "\n",
    "Desarrolle un agente básico capaz de resolver un problema simplificado del Proceso de Decisión de Markov (MDP).\n",
    "Considere utilizar un ejemplo bien conocido como el entorno 'Frozen Lake'. Proporcione el código Python para el\n",
    "proceso de toma de decisiones del agente basándose únicamente en los principios de los procesos de decisión de\n",
    "Markov. Recuerde que para este tipo de problema, el ambiente es una matriz de 4x4, y las acciones, pueden ser\n",
    "moverse hacia arriba, abajo, derecha, izquierda. Considere que el punto inicial siempre estará en la esquina opuesta\n",
    "del punto de meta. Es decir, puede tener hasta 4 configuraciones diferentes. Por ejemplo, el punto inicial puede estar\n",
    "en la coordenada (0, 0) y el punto de meta en la coordenada en la coordenada (3, 3). Además, la posición de los hoyos\n",
    "debe ser determinada aleatoriamente y no debe superar el ser más de 3. Es decir, si aleatoriamente se decide que\n",
    "sean 2 posiciones de hoyo, las coordenadas de estas deben ser determinadas de forma aleatoria. Asegúrese de usar\n",
    "“seed” para que sus resultados sean consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 524.3/721.7 kB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 721.7/721.7 kB 2.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy>=1.18.0 (from gym)\n",
      "  Downloading numpy-2.2.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Downloading numpy-2.2.3-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.6 MB 6.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.6/12.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.3/12.6 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.3/12.6 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.6/12.6 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.6/12.6 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.6/12.6 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.9/12.6 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.1/12.6 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.4/12.6 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.4/12.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.0/12.6 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.2/12.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.6 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 723.5 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827705 sha256=f9e9d21fcb57dbef1ce62f99a7e5efecc9cd3d988292d67fa0295476698a542e\n",
      "  Stored in directory: c:\\users\\nel20\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\1d\\34\\c6\\856a1e1eff47d8f18545c833b6138ae1e9f53c7de9bcc5f31d\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, numpy, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.1.1 gym-0.26.2 gym_notices-0.0.8 numpy-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\nel20\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess():\n",
    "    def __init__(self, num_states, num_actions, dynamics_fn):\n",
    "        \"\"\"\n",
    "        Initializes an object representing a Markov Decision Process.\n",
    "        Assumes the reward is deterministic for a given state.\n",
    "        \"\"\"\n",
    "        assert num_states > 0 and num_actions > 0\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        # P[s][a] represents a list of possible transistions given state s and a.\n",
    "        # each transistion is expected as a list/tuple: \n",
    "        # [prob_next_state, next_state, reward, is_terminal]\n",
    "        self.P = dynamics_fn\n",
    "        # sanity checks\n",
    "        self.__verify()\n",
    "    \n",
    "    def __verify(self):\n",
    "        assert len(self.P) == self.num_states\n",
    "        for s in self.P.keys():\n",
    "             assert len(self.P[s]) == self.num_actions\n",
    "        for s in self.P.keys():\n",
    "            for a in self.P[s].keys():\n",
    "                transitions = self.P[s][a]\n",
    "                p_sum = sum([t[0] for t in transitions])\n",
    "                assert p_sum <= 1 and p_sum > 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResetNeeded",
     "evalue": "Cannot call `env.render()` before calling `env.reset()`, if this is a intended action, set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResetNeeded\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m MDP = MarkovDecisionProcess\n\u001b[32m      3\u001b[39m env = gym.make(\u001b[33m'\u001b[39m\u001b[33mFrozenLake-v1\u001b[39m\u001b[33m'\u001b[39m, is_slippery=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m mdp = MDP(env.observation_space.n, env.action_space.n, env.unwrapped.P)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumber of states \u001b[39m\u001b[33m\"\u001b[39m, mdp.num_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\gym\\core.py:329\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\n\u001b[32m    326\u001b[39m     \u001b[38;5;28mself\u001b[39m, *args, **kwargs\n\u001b[32m    327\u001b[39m ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[32m    328\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\gym\\wrappers\\order_enforcing.py:47\u001b[39m, in \u001b[36mOrderEnforcing.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Renders the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m     )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.render(*args, **kwargs)\n",
      "\u001b[31mResetNeeded\u001b[39m: Cannot call `env.render()` before calling `env.reset()`, if this is a intended action, set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper."
     ]
    }
   ],
   "source": [
    "MDP = MarkovDecisionProcess\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.render()\n",
    "mdp = MDP(env.observation_space.n, env.action_space.n, env.unwrapped.P)\n",
    "\n",
    "print(\"Number of states \", mdp.num_states)\n",
    "print(\"Number of actions \", mdp.num_actions)\n",
    "\n",
    "sample_actions = {'LEFT':0, 'UP':3}\n",
    "sample_states = [0,11,15]\n",
    "for s in sample_states:\n",
    "    for a in sample_actions.keys():\n",
    "        print(f\"Transitions for state {s} and action {a} are\\n \", mdp.P[s][sample_actions[a]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografía:\n",
    "\n",
    "- Buczyński, R. (2023, 21 septiembre). Understanding Markov Decision Processes - Python in Plain English. Medium. https://python.plainenglish.io/understanding-markov-decision-processes-17e852cd9981"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
