{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Preguntas Analíticas\n",
    "\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de\n",
    "Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones\n",
    "para la toma de decisiones.\n",
    "\n",
    "El proceso de decisión de Markov en general es un modelo matemático bastante útil para la simulación de toma de decisiones buscando maximizar la ganancia de tomar dichas decisiones. Para que el modelo funcionem se establecen ciertas reglas y suposiciones para que el modelo pueda ser aplicable. Los supuestos subyacentes, son este conjunto de reglas implicitas que simplifican la realidad y el ambiente para poder hacer un Proceso de Decisión de Markov. Por ejemplo, la propiedad de Markov (el supuesto más importante), establece que el fútutro solo depende del estado actual y no del pasado. De manera práctica quiere decir que los procesos y decisiones no tienen memoria y para tomar una decisión solo se toma en consideración el estado actual. Esto es útil para simplificar los cálculos y análisis pero puede presentar algunas dificultades si el problema que se desea modelar es más complejo. Además la propiedad de markov también hay otros supuestos como:\n",
    "\n",
    "* El entorno debe ser completamente observable\n",
    "* Los estados y acciones son finitas\n",
    "* Las recompensas y transiciones no cambian \n",
    "* Se conoce el factor de descuento y este no cambia\n",
    "\n",
    "Como se mencionó anteriromente, estas suposiciones permiten manejar problemas de manera relativamente sencilla usando modelos con estados para tomar decisiones. Estas restricciones simplifican los problemas para que sean más sencillos de modelar, pero todas estas simplificaciones no siempre serán posibles dependiendo de la complejidad del problema. En algunos puede que alguno de los supuestos no se cumplan. Por ejemplo el supuesto principal \"el futuro solo depende del estado actual\", puede no cumplirse en muchos casos. En muchas situaciones las decisiones de un agente pueden afectar al ambiente y el ambiente puede afectar al agente y por ende las decisiones pasadas pueden afectar a la toma decisiones para el futuro. El futuro no siempre depende del problema actual, en distintos casos puede el futuro puede depender del estado actual y las decisiones tomadas anteriormente. \n",
    "\n",
    "Digamos por ejemplo un algoritmo de recomendación, las decisiones pasadas (compras) influyen grandemente en las decisiones futuras para la toma de decisión y predicciones. Otro ejemplo donde estos supuestos pueden no cumplirse es en situaciones donde no se tiene un ambiente completamente observable, el ambiente no siempre será conocido al 100%. Por ejemplo en un juego de poker, no se conoce completamente el ambiente desde el punto de vista de un jugador, ya que no se conocen las cartas de los demás. En este ejemplo del Poker también habría importancia en las decisiones pasadas y no solo en el estado actual para tomar decisiones. Un ejemplo más podría ser robots y su control. Aunque un robot puede tener distintos estados, no se limita a un cantidad finita o discreta de datos. Los robots en un estado podrían tener variables continuas relacionadas a posición y velocidad por ejemplo, además modelar todos los posibles estados en los que un robot puede estar en un ambiente sería prácticamente imposible o al menos exageradamente largo dependiendo del entorno. Finalmente, en el mundo real y distintos modelos, como lo puede ser un mercado financiero, las recompensas/ganancias pueden evolucionar y cambiar en el tiempo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice\n",
    "estrategias para una toma de decisiones sólida en entornos inciertos.\n",
    "\n",
    "La incertidumbre es uno de los desafíos grandes de trabajar con procesos de decisión de Markov, ya que se espera tener un ambiente completamente observable. Pero la incertidumbre es común en distintos tipos de problemas y en el caso de los MDP, la incertidumbre puede darse en distintos casos. Por ejemplo incertidumbre en las transiciones, cuando no se tiene la probabilidad de algún evento o decisión; incertidumbre en la recompensa, si no son conocidas con exactitud o esta no es constante; incertidumbre en los estados, si no se tiene completa observabilidad del entorno se deben tomar decisiones con información incompleta y finalmente puede haber incertidumbre en el descuento si este cambia, es demasiado complicado para deteminar la importancia de las recompensas inmediatas vs las futuras o bien si no se sabe cuál es el descuetno como tal. PRácticamente cada uno de los componentes del proceso de decisión de Markov puede introducir cierta incertidumbre dentro del problema dependiendo de la situación que se desea modelar. \n",
    "\n",
    "La incertidumbre en algunos casos no es un error, sino un parte importante del entorno que debe ser modelado para poder tomar decisiones correctas. Una e las primeras alternativas sería no solo usar MDP sino POMDP, una vairación del proceso de deciósn de markov, pero para ambientes parcialmente observables. Por definición el POMDP es un modelo porbabilístico para la toma de decisiones bajo incertidumbre. Dicha incertidumbre se modela como  una distribución de probabilidad ala sque comunmente se les llama \"creencias\". Estas creencias ayudan al agente a poder tomar decisiones incluso bajo incertidumbre. Lo interesante es que el modelo puede incluso tener incertidumbre sobre el estado en el que está, pero con ayuda de la dsitribución de probabilidad sobre los posibles estados es que el modelo puede tener una \"creencia\" (belief en inglés) sobre donde puede estar. La creencia es actualizada usando la regla de bayes a medida que se hace nuevas observaciones. La creencia se puede modelar para ser aplicado en los estados u otros datos para poder tomar decisiones en ambientes ruidosas con incertidumbre y falta de datos.\n",
    "\n",
    "Referencias:\n",
    "https://numerentur.org/proceso-de-decision-pomdp/\n",
    "https://www.geeksforgeeks.org/partially-observable-markov-decision-process-pomdp-in-ai/#what-is-partially-observable-markov-decision-process-pomdp"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
